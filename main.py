import os
import json
from typing import List, Dict, Any

import streamlit as st
from openai import OpenAI

# ----------------------------
# App Config
# ----------------------------
st.set_page_config(page_title="Student Writing Evaluator (LLM)", page_icon="üìù", layout="wide")

# ----------------------------
# Sidebar: Settings
# ----------------------------
st.sidebar.title("‚öôÔ∏è Settings")
api_key = (
    st.sidebar.text_input("OpenAI API Key", type="password")
    or os.getenv("OPENAI_API_KEY")
    or st.secrets.get("OPENAI_API_KEY", None)
)
model_name = st.sidebar.text_input("Model", value="gpt-4o-mini")

st.sidebar.markdown(
    """
    **How to use**
    1) Enter your API key.  
    2) Choose a topic and paste a student's writing.  
    3) Click **Evaluate**.
    """
)

# ----------------------------
# Topics
# ----------------------------
DEFAULT_TOPICS = [
    "Does technological development change society? / Í∏∞Ïà† Î∞úÏ†ÑÏùÄ ÏÇ¨ÌöåÎ•º Î≥ÄÌôîÏãúÌÇ§ÎäîÍ∞Ä?",
    "Wild Robot: Technology & Nature Coexistence / ÏôÄÏùºÎìú Î°úÎ¥á: Í∏∞Ïà†Í≥º ÏûêÏó∞Ïùò Í≥µÏ°¥",
    "Digital Citizenship & Online Etiquette / ÎîîÏßÄÌÑ∏ ÏãúÎØºÏÑ±Í≥º Ïò®ÎùºÏù∏ ÏòàÏ†à",
    "AI Ethics in School Life / ÌïôÍµêÏÉùÌôú ÏÜç AI Ïú§Î¶¨",
]

st.title("üìù Student Writing Evaluator ‚Ä¢ Streamlit + OpenAI")

colA, colB = st.columns([2, 1])
with colA:
    topic = st.selectbox(
        "Select a topic (or choose 'Custom') / Ï£ºÏ†úÎ•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî",
        options=["Custom / ÏÇ¨Ïö©Ïûê Ï†ïÏùò"] + DEFAULT_TOPICS,
        index=1,
    )
    custom_topic = ""
    if topic.startswith("Custom"):
        custom_topic = st.text_input("Enter your custom topic / ÏÇ¨Ïö©Ïûê Ï†ïÏùò Ï£ºÏ†úÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî")

with colB:
    lang = st.selectbox("Feedback language / ÌîºÎìúÎ∞± Ïñ∏Ïñ¥", ["Korean", "English", "Bilingual"])

essay = st.text_area(
    "Paste student's writing here / ÌïôÏÉù Í∏ÄÏùÑ Î∂ôÏó¨ÎÑ£ÏúºÏÑ∏Ïöî",
    height=280,
    placeholder=(
        "Ïòà: Í∏∞Ïà† Î∞úÏ†ÑÏùÄ ÏÇ¨ÌöåÎ•º Ïñ¥ÎñªÍ≤å Î∞îÍæ∏ÎäîÍ∞ÄÏóê ÎåÄÌïú ÎÇòÏùò ÏÉùÍ∞ÅÏùÄ‚Ä¶\n"
        "(Ï≤†Ïûê¬∑ÎùÑÏñ¥Ïì∞Í∏∞¬∑Î¨∏Ïû• Ïò§Î•òÍ∞Ä ÏûàÏñ¥ÎèÑ Í¥úÏ∞ÆÏïÑÏöî. Î™®Îëê Ïû°ÏïÑ ÎìúÎ¶ΩÎãàÎã§.)"
    ),
)

# ----------------------------
# Rubric (from user spec)
# ----------------------------
RUBRIC = [
    {
        "id": "understanding",
        "name": "1) ÎÇ¥Ïö© Ïù¥Ìï¥ / Understanding the contents",
        "desc": "ÏûëÌíà/Ï£ºÏ†úÏùò ÌïµÏã¨ ÎÇ¥Ïö©, Îì±Ïû•Ïù∏Î¨ºÏùò ÎßêÍ≥º ÌñâÎèô, Í∞úÎÖêÏùÑ Ï†ïÌôïÌûà Ïù¥Ìï¥ÌïòÍ≥† Î∞òÏòÅÌñàÎäîÍ∞Ä?",
    },
    {
        "id": "ideas_arguments",
        "name": "2) ÏïÑÏù¥ÎîîÏñ¥¬∑Ï£ºÏû• / Ideas & Argumentation",
        "desc": "Ï£ºÏû•Í≥º Í∑ºÍ±∞Í∞Ä Î™ÖÌôïÌïòÍ≥† ÌÜµÏ∞∞Ïù¥ ÏûàÎäîÍ∞Ä? ÏÇ¨Î°Ä¬∑Îç∞Ïù¥ÌÑ∞¬∑Ïù∏Ïö© Îì±ÏúºÎ°ú Îí∑Î∞õÏπ®ÌñàÎäîÍ∞Ä?",
    },
    {
        "id": "organization",
        "name": "3) Íµ¨ÏÑ±¬∑Ï†ÑÍ∞ú / Composition & Organization",
        "desc": "ÏÑúÎ°†-Î≥∏Î°†-Í≤∞Î°†Ïùò Íµ¨Ï°∞, Î¨∏Îã® Í∞Ñ ÎÖºÎ¶¨ ÌùêÎ¶Ñ, Ïó∞Í≤∞Ïñ¥ ÏÇ¨Ïö©Ïù¥ ÏûêÏó∞Ïä§Îü¨Ïö¥Í∞Ä?",
    },
    {
        "id": "expression",
        "name": "4) ÌëúÌòÑ¬∑Î¨∏Ïû• / Expression & Sentences",
        "desc": "Ïñ¥Ìúò ÏÑ†ÌÉù, ÎßûÏ∂§Î≤ï¬∑ÎùÑÏñ¥Ïì∞Í∏∞, Î¨∏Ïû• Îã§ÏñëÏÑ±(Îã®Î¨∏/Î≥µÎ¨∏/Î≥ëÎ†¨)Í≥º Î™ÖÎ£åÏÑ±Ïù¥ Ï†ÅÏ†àÌïúÍ∞Ä?",
    },
    {
        "id": "attitude_integrity",
        "name": "5) ÌÉúÎèÑ¬∑ÏÑ±Ïã§ÏÑ± / Attitude & Integrity",
        "desc": "Î∂ÑÎüâ, Ï£ºÍ¥ÄÏ†Å ÏÑ±Ï∞∞Ïùò ÍπäÏù¥, Í≥ºÏ†ú ÏßÄÏπ® Ï§ÄÏàò, Ïù∏Ïö© ÌëúÍ∏∞ Îì±ÏùÑ ÏÑ±Ïã§Ìûà ÏàòÌñâÌñàÎäîÍ∞Ä?",
    },
]

RUBRIC_SCALE = {
    5: "ÌÉÅÏõî/Excellent",
    4: "Ïö∞Ïàò/Strong",
    3: "Î≥¥ÌÜµ/Adequate",
    2: "ÎØ∏Ìù°/Limited",
    1: "Î∂ÄÏ°±/Weak",
}

# ----------------------------
# Prompt builder
# ----------------------------
SYSTEM_PROMPT = (
    "You are a meticulous Korean middle-school writing evaluator and editor. "
    "Follow the rubric strictly and produce exhaustive, actionable feedback. "
    "When correcting grammar/spelling/spacing, list *every* error with 'Original ‚Üí Amended' and a short reason. "
)

USER_INSTRUCTIONS_TEMPLATE = """
You will evaluate a student's essay according to the following rubric and output **valid JSON only**.

Topic: "{topic}"

Rubric (5-point scale per item):
- 1) Understanding the contents ‚Äî ÏûëÌíà/Ï£ºÏ†ú Ïù¥Ìï¥ÎèÑ
- 2) Ideas & Argumentation ‚Äî Ï£ºÏû•¬∑Í∑ºÍ±∞¬∑ÌÜµÏ∞∞
- 3) Composition & Organization ‚Äî Íµ¨Ï°∞¬∑ÎÖºÎ¶¨ ÌùêÎ¶Ñ
- 4) Expression & Sentences ‚Äî Ïñ¥Ìúò¬∑ÎßûÏ∂§Î≤ï¬∑Î¨∏Ïû• Îã§ÏñëÏÑ±
- 5) Attitude & Integrity ‚Äî Î∂ÑÎüâ¬∑ÏÑ±Ï∞∞¬∑ÏßÄÏπ® Ï§ÄÏàò

**Feedback requirements:**
1) Corrections (spell, spacing, grammar): list every error as objects with fields `original`, `amended`, `reason`.  
2) Praise: concrete compliments referencing exact phrases/parts from the student's text (no vague praise).  
3) Improvements: clear, specific fixes with a concrete `rewrite_example` for each issue.  
4) Scores: integer 1‚Äì5 for each rubric item with short `explanation` per item, and `overall_score` (1‚Äì5).  
5) Overall comment: brief summary.

**Language for outputs:** {lang}

Student essay begins below (between triple backticks):
```essay
{essay}
```

Return **ONLY** a single JSON object with this schema:
{
  "scores": {
    "understanding": {"score": int, "explanation": string},
    "ideas_arguments": {"score": int, "explanation": string},
    "organization": {"score": int, "explanation": string},
    "expression": {"score": int, "explanation": string},
    "attitude_integrity": {"score": int, "explanation": string}
  },
  "overall_score": int,
  "corrections": [ {"original": string, "amended": string, "reason": string}, ... ],
  "praise": [ string, ... ],
  "improvements": [ {"issue": string, "suggestion": string, "rewrite_example": string}, ... ],
  "overall_comment": string
}
"""


def build_user_prompt(topic_label: str, custom_topic: str, essay: str, lang_choice: str) -> str:
    topic_final = custom_topic.strip() if topic_label.startswith("Custom") else topic_label
    lang_map = {
        "Korean": "Korean (ÌïúÍµ≠Ïñ¥)",
        "English": "English",
        "Bilingual": "Bilingual (ÌïúÍµ≠Ïñ¥+English)"
    }
    return USER_INSTRUCTIONS_TEMPLATE.format(
        topic=topic_final,
        essay=essay,
        lang=lang_map.get(lang_choice, "Korean"),
    )


# ----------------------------
# OpenAI Call
# ----------------------------

def call_openai(prompt: str, api_key: str, model: str) -> Dict[str, Any]:
    client = OpenAI(api_key=api_key)
    completion = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
        response_format={"type": "json_object"},
    )
    text = completion.choices[0].message.content
    return json.loads(text)


# ----------------------------
# UI: Evaluate Button
# ----------------------------

def render_rubric_table(result: Dict[str, Any]):
    st.subheader("üìä Rubric Scores (1‚Äì5)")
    rows = []
    for r in RUBRIC:
        item = result.get("scores", {}).get(r["id"], {})
        score = item.get("score", "-")
        expl = item.get("explanation", "")
        rows.append({"Item": r["name"], "Score": score, "Band": RUBRIC_SCALE.get(score, "-"), "Explanation": expl})
    st.dataframe(rows, use_container_width=True, hide_index=True)


def render_corrections(result: Dict[str, Any]):
    st.subheader("üõ†Ô∏è Corrections (Original ‚Üí Amended)")
    corrections: List[Dict[str, str]] = result.get("corrections", [])
    if not corrections:
        st.info("No corrections detected.")
        return
    for i, c in enumerate(corrections, start=1):
        with st.expander(f"Correction {i}"):
            st.markdown(f"**Original**: {c.get('original','')}")
            st.markdown(f"**Amended**: {c.get('amended','')}")
            st.markdown(f"**Reason**: {c.get('reason','')}")


def render_list_section(title: str, items: List[str], empty_msg: str):
    st.subheader(title)
    if not items:
        st.info(empty_msg)
        return
    for it in items:
        st.markdown(f"- {it}")


if st.button("üöÄ Evaluate", type="primary", use_container_width=True):
    if not api_key:
        st.error("Please provide an OpenAI API key in the sidebar.")
    elif not essay.strip():
        st.warning("Please paste a student's essay.")
    else:
        with st.spinner("Evaluating with LLM‚Ä¶"):
            try:
                prompt = build_user_prompt(topic, custom_topic, essay, lang)
                result = call_openai(prompt, api_key, model_name)

                # Top summary
                st.success("Evaluation complete.")
                st.markdown(f"**Overall Score**: {result.get('overall_score', '-')}/5")
                st.markdown(result.get("overall_comment", ""))

                # Rubric table
                render_rubric_table(result)

                # Praise
                render_list_section("üåü What You Did Well (Praise)", result.get("praise", []), "No praise items returned.")

                # Improvements
                improvements = result.get("improvements", [])
                st.subheader("üîß Improvements (with concrete rewrites)")
                if not improvements:
                    st.info("No improvement suggestions returned.")
                else:
                    for i, imp in enumerate(improvements, start=1):
                        with st.expander(f"Improvement {i}"):
                            st.markdown(f"**Issue**: {imp.get('issue','')}")
                            st.markdown(f"**Suggestion**: {imp.get('suggestion','')}")
                            st.markdown(f"**Rewrite Example**:\n\n> {imp.get('rewrite_example','')}")

                # Corrections
                render_corrections(result)

                # Download raw JSON
                st.download_button(
                    label="‚¨áÔ∏è Download JSON Feedback",
                    data=json.dumps(result, ensure_ascii=False, indent=2).encode("utf-8"),
                    file_name="evaluation_feedback.json",
                    mime="application/json",
                )

            except Exception as e:
                st.error(f"Error: {e}")

# Footer
st.markdown("---")
st.caption(
    "¬© 2025 Student Writing Evaluator ‚Ä¢ Built with Streamlit + OpenAI. This tool follows your rubric and generates exhaustive, example-based feedback."
)
